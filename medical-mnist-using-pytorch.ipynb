{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":1099232,"sourceType":"datasetVersion","datasetId":614679}],"dockerImageVersionId":30698,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/sarthaksshukla/medical-mnist-using-se-resnet-99-8-accuracy?scriptVersionId=181265892\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"code","source":"!pip install torch-summary","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-06-03T12:17:38.202314Z","iopub.execute_input":"2024-06-03T12:17:38.202692Z","iopub.status.idle":"2024-06-03T12:17:52.061901Z","shell.execute_reply.started":"2024-06-03T12:17:38.202661Z","shell.execute_reply":"2024-06-03T12:17:52.060734Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Collecting torch-summary\n  Downloading torch_summary-1.4.5-py3-none-any.whl.metadata (18 kB)\nDownloading torch_summary-1.4.5-py3-none-any.whl (16 kB)\nInstalling collected packages: torch-summary\nSuccessfully installed torch-summary-1.4.5\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch\nimport os\nimport shutil\nimport random\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torchvision.transforms as transforms\nfrom torchvision.datasets import ImageFolder\nfrom torch.utils.data import DataLoader\nfrom fastprogress import master_bar,progress_bar\nfrom tqdm import tqdm\nfrom torchsummary import summary","metadata":{"execution":{"iopub.status.busy":"2024-06-03T12:18:27.57644Z","iopub.execute_input":"2024-06-03T12:18:27.577246Z","iopub.status.idle":"2024-06-03T12:18:33.710473Z","shell.execute_reply.started":"2024-06-03T12:18:27.577216Z","shell.execute_reply":"2024-06-03T12:18:33.709477Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"## Building the model","metadata":{}},{"cell_type":"code","source":"class Conv2d(nn.Module):\n    def __init__(self,in_channels,out_channels,kernel_size = 3):\n        super().__init__()\n        self.model = nn.Sequential(*[\n            nn.Conv2d(in_channels,out_channels,kernel_size = kernel_size,stride = 1,padding = (kernel_size - 1) // 2),\n            nn.BatchNorm2d(num_features = out_channels),\n            nn.ReLU()\n        ])\n    \n    def forward(self,x):\n        return self.model(x)","metadata":{"execution":{"iopub.status.busy":"2024-06-03T12:18:33.712401Z","iopub.execute_input":"2024-06-03T12:18:33.71311Z","iopub.status.idle":"2024-06-03T12:18:33.719977Z","shell.execute_reply.started":"2024-06-03T12:18:33.713083Z","shell.execute_reply":"2024-06-03T12:18:33.718298Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"class Dense(nn.Module):\n    def __init__(self,in_features,out_features,activation = nn.ReLU()):\n        super().__init__()\n        self.model = nn.Sequential(*[\n            nn.Linear(in_features,out_features),\n            activation\n        ])\n    \n    def forward(self,x):\n        return self.model(x)","metadata":{"execution":{"iopub.status.busy":"2024-06-03T12:18:33.721334Z","iopub.execute_input":"2024-06-03T12:18:33.721714Z","iopub.status.idle":"2024-06-03T12:18:33.73604Z","shell.execute_reply.started":"2024-06-03T12:18:33.721681Z","shell.execute_reply":"2024-06-03T12:18:33.735178Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"class ScoreBlock(nn.Module):\n    def __init__(self,channels):\n        super().__init__()\n        self.model = nn.Sequential(*[\n            nn.AdaptiveAvgPool3d((1,1,channels)),\n            nn.Flatten(),\n            Dense(channels,channels),\n            nn.Dropout(0.25),\n            Dense(channels,channels),\n            nn.Dropout(0.25),\n            nn.Sigmoid()\n        ])\n    \n    def forward(self,x):\n        score = self.model(x)\n        return torch.reshape(score,(*score.shape,1,1)) * x","metadata":{"execution":{"iopub.status.busy":"2024-06-03T12:34:41.663436Z","iopub.execute_input":"2024-06-03T12:34:41.664357Z","iopub.status.idle":"2024-06-03T12:34:41.67141Z","shell.execute_reply.started":"2024-06-03T12:34:41.664318Z","shell.execute_reply":"2024-06-03T12:34:41.670255Z"},"trusted":true},"execution_count":51,"outputs":[]},{"cell_type":"code","source":"class ConvStack(nn.Module):\n    def __init__(self,in_channels,num_conv = 3):\n        super().__init__()\n        self.model = nn.Sequential(*[\n            Conv2d(in_channels,in_channels // 2,kernel_size = 1),\n            *[Conv2d(in_channels // 2,in_channels // 2) for _ in range(num_conv - 1)],\n            Conv2d(in_channels // 2,2 * in_channels),\n            nn.Dropout(0.25)\n        ])\n        self.out_channels = in_channels * 2\n        self.add_helper = Conv2d(in_channels,self.out_channels,kernel_size = 1)\n        self.score_block = ScoreBlock(self.out_channels)\n    \n    def forward(self,x):\n        return F.relu(self.score_block(self.model(x) + self.add_helper(x)))","metadata":{"execution":{"iopub.status.busy":"2024-06-03T12:34:42.019425Z","iopub.execute_input":"2024-06-03T12:34:42.02005Z","iopub.status.idle":"2024-06-03T12:34:42.027805Z","shell.execute_reply.started":"2024-06-03T12:34:42.020018Z","shell.execute_reply":"2024-06-03T12:34:42.026702Z"},"trusted":true},"execution_count":52,"outputs":[]},{"cell_type":"code","source":"class Pool(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.model = nn.Sequential(*[\n            nn.AvgPool2d(kernel_size = 2,stride = 2),\n            nn.ReLU(),\n        ])\n    \n    def forward(self,x):\n        return self.model(x)","metadata":{"execution":{"iopub.status.busy":"2024-06-03T12:26:48.913065Z","iopub.execute_input":"2024-06-03T12:26:48.91343Z","iopub.status.idle":"2024-06-03T12:26:48.919266Z","shell.execute_reply.started":"2024-06-03T12:26:48.913398Z","shell.execute_reply":"2024-06-03T12:26:48.918174Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"class FCN(nn.Module):\n    def __init__(self,out_channels,num_classes,units = 4096):\n        super().__init__()\n        self.model = nn.Sequential(*[\n            nn.Flatten(),\n            Dense(out_channels,units),\n            Dense(units,units),\n            Dense(units,units),\n            Dense(units,num_classes,activation = nn.Softmax(dim = -1))\n        ])\n    \n    def forward(self,x):\n        return self.model(x)","metadata":{"execution":{"iopub.status.busy":"2024-06-03T12:34:57.390436Z","iopub.execute_input":"2024-06-03T12:34:57.390812Z","iopub.status.idle":"2024-06-03T12:34:57.397361Z","shell.execute_reply.started":"2024-06-03T12:34:57.390775Z","shell.execute_reply":"2024-06-03T12:34:57.396268Z"},"trusted":true},"execution_count":56,"outputs":[]},{"cell_type":"code","source":"class Resnet(nn.Module):\n    def __init__(self,image_size,num_classes,units = 4096,num_conv = 5):\n        super().__init__()\n        height,width = image_size\n        channels = 3\n        self.model = []\n        while height > 1:\n            self.model.extend([\n                ConvStack(channels,num_conv),\n                Pool()\n            ])\n            channels = self.model[-2].out_channels\n            height //= 2\n        self.model.append(FCN(channels,num_classes,units))\n        self.model = nn.Sequential(*self.model)\n    \n    def forward(self,x):\n        return self.model(x)","metadata":{"execution":{"iopub.status.busy":"2024-06-03T12:26:49.355888Z","iopub.execute_input":"2024-06-03T12:26:49.356238Z","iopub.status.idle":"2024-06-03T12:26:49.363364Z","shell.execute_reply.started":"2024-06-03T12:26:49.356208Z","shell.execute_reply":"2024-06-03T12:26:49.362463Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"markdown","source":"# Handling the data","metadata":{}},{"cell_type":"code","source":"image_data_path = \"/kaggle/input/medical-mnist\"","metadata":{"execution":{"iopub.status.busy":"2024-06-03T12:26:49.959822Z","iopub.execute_input":"2024-06-03T12:26:49.960169Z","iopub.status.idle":"2024-06-03T12:26:49.964328Z","shell.execute_reply.started":"2024-06-03T12:26:49.960139Z","shell.execute_reply":"2024-06-03T12:26:49.963374Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"classes = os.listdir(image_data_path)\nprint(classes)","metadata":{"execution":{"iopub.status.busy":"2024-06-03T12:26:50.270545Z","iopub.execute_input":"2024-06-03T12:26:50.271153Z","iopub.status.idle":"2024-06-03T12:26:50.282368Z","shell.execute_reply.started":"2024-06-03T12:26:50.27112Z","shell.execute_reply":"2024-06-03T12:26:50.281544Z"},"trusted":true},"execution_count":25,"outputs":[{"name":"stdout","text":"['AbdomenCT', 'BreastMRI', 'Hand', 'CXR', 'HeadCT', 'ChestCT']\n","output_type":"stream"}]},{"cell_type":"code","source":"model_data_parent_dir = \"/kaggle/data\"\ntrain_path = os.path.join(model_data_parent_dir,\"train\")\nvalid_path = os.path.join(model_data_parent_dir,\"valid\")\ntest_path = os.path.join(model_data_parent_dir,\"test\")","metadata":{"execution":{"iopub.status.busy":"2024-06-03T12:26:50.607377Z","iopub.execute_input":"2024-06-03T12:26:50.607741Z","iopub.status.idle":"2024-06-03T12:26:50.61237Z","shell.execute_reply.started":"2024-06-03T12:26:50.60771Z","shell.execute_reply":"2024-06-03T12:26:50.611534Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"def create_paths(parent_dir,classes):\n    types = ['train','valid','test']\n    if os.path.exists(parent_dir) is False:\n        os.mkdir(parent_dir)\n        \n        for t in types:\n            type_path = os.path.join(parent_dir,t)\n            os.mkdir(type_path)\n            print(f\"{type_path} has been constructed\")\n            for target in classes:\n                target_path = os.path.join(type_path,target)\n                os.mkdir(target_path)\n                print(f\"{target_path} has been constructed\")","metadata":{"execution":{"iopub.status.busy":"2024-06-03T12:26:50.971725Z","iopub.execute_input":"2024-06-03T12:26:50.97209Z","iopub.status.idle":"2024-06-03T12:26:50.978428Z","shell.execute_reply.started":"2024-06-03T12:26:50.972059Z","shell.execute_reply":"2024-06-03T12:26:50.977489Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"create_paths(model_data_parent_dir,classes)","metadata":{"execution":{"iopub.status.busy":"2024-06-03T12:26:51.335621Z","iopub.execute_input":"2024-06-03T12:26:51.336007Z","iopub.status.idle":"2024-06-03T12:26:51.342609Z","shell.execute_reply.started":"2024-06-03T12:26:51.335976Z","shell.execute_reply":"2024-06-03T12:26:51.341677Z"},"trusted":true},"execution_count":28,"outputs":[{"name":"stdout","text":"/kaggle/data/train has been constructed\n/kaggle/data/train/AbdomenCT has been constructed\n/kaggle/data/train/BreastMRI has been constructed\n/kaggle/data/train/Hand has been constructed\n/kaggle/data/train/CXR has been constructed\n/kaggle/data/train/HeadCT has been constructed\n/kaggle/data/train/ChestCT has been constructed\n/kaggle/data/valid has been constructed\n/kaggle/data/valid/AbdomenCT has been constructed\n/kaggle/data/valid/BreastMRI has been constructed\n/kaggle/data/valid/Hand has been constructed\n/kaggle/data/valid/CXR has been constructed\n/kaggle/data/valid/HeadCT has been constructed\n/kaggle/data/valid/ChestCT has been constructed\n/kaggle/data/test has been constructed\n/kaggle/data/test/AbdomenCT has been constructed\n/kaggle/data/test/BreastMRI has been constructed\n/kaggle/data/test/Hand has been constructed\n/kaggle/data/test/CXR has been constructed\n/kaggle/data/test/HeadCT has been constructed\n/kaggle/data/test/ChestCT has been constructed\n","output_type":"stream"}]},{"cell_type":"code","source":"def move_data(src_dir,tgt_dir,mode,size):\n    for target in os.listdir(src_dir):\n        target_dir = os.path.join(src_dir,target)\n        target_files = random.sample(os.listdir(target_dir),size)\n        dest_dir = os.path.join(tgt_dir,target)\n        print(f\"Moving {mode} images for class: {target}\")\n        \n        for target_file in tqdm(target_files):\n            target_file_path = os.path.join(target_dir,target_file)\n            shutil.copy(target_file_path,dest_dir)","metadata":{"execution":{"iopub.status.busy":"2024-06-03T12:26:51.700352Z","iopub.execute_input":"2024-06-03T12:26:51.701166Z","iopub.status.idle":"2024-06-03T12:26:51.707248Z","shell.execute_reply.started":"2024-06-03T12:26:51.701134Z","shell.execute_reply":"2024-06-03T12:26:51.706341Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"move_data(image_data_path,train_path,\"train\",6000)","metadata":{"execution":{"iopub.status.busy":"2024-06-03T12:26:52.142157Z","iopub.execute_input":"2024-06-03T12:26:52.142502Z","iopub.status.idle":"2024-06-03T12:29:32.899548Z","shell.execute_reply.started":"2024-06-03T12:26:52.142473Z","shell.execute_reply":"2024-06-03T12:29:32.898561Z"},"trusted":true},"execution_count":30,"outputs":[{"name":"stdout","text":"Moving train images for class: AbdomenCT\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 6000/6000 [00:27<00:00, 222.15it/s]\n","output_type":"stream"},{"name":"stdout","text":"Moving train images for class: BreastMRI\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 6000/6000 [00:25<00:00, 236.61it/s]\n","output_type":"stream"},{"name":"stdout","text":"Moving train images for class: Hand\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 6000/6000 [00:26<00:00, 226.44it/s]\n","output_type":"stream"},{"name":"stdout","text":"Moving train images for class: CXR\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 6000/6000 [00:27<00:00, 220.73it/s]\n","output_type":"stream"},{"name":"stdout","text":"Moving train images for class: HeadCT\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 6000/6000 [00:26<00:00, 226.62it/s]\n","output_type":"stream"},{"name":"stdout","text":"Moving train images for class: ChestCT\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 6000/6000 [00:25<00:00, 231.06it/s]\n","output_type":"stream"}]},{"cell_type":"code","source":"move_data(image_data_path,valid_path,\"valid\",2000)","metadata":{"execution":{"iopub.status.busy":"2024-06-03T12:29:32.901567Z","iopub.execute_input":"2024-06-03T12:29:32.90209Z","iopub.status.idle":"2024-06-03T12:30:00.26717Z","shell.execute_reply.started":"2024-06-03T12:29:32.902055Z","shell.execute_reply":"2024-06-03T12:30:00.266186Z"},"trusted":true},"execution_count":31,"outputs":[{"name":"stdout","text":"Moving valid images for class: AbdomenCT\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 2000/2000 [00:04<00:00, 434.01it/s]\n","output_type":"stream"},{"name":"stdout","text":"Moving valid images for class: BreastMRI\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 2000/2000 [00:04<00:00, 499.53it/s]\n","output_type":"stream"},{"name":"stdout","text":"Moving valid images for class: Hand\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 2000/2000 [00:04<00:00, 427.07it/s]\n","output_type":"stream"},{"name":"stdout","text":"Moving valid images for class: CXR\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 2000/2000 [00:04<00:00, 427.42it/s]\n","output_type":"stream"},{"name":"stdout","text":"Moving valid images for class: HeadCT\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 2000/2000 [00:04<00:00, 427.86it/s]\n","output_type":"stream"},{"name":"stdout","text":"Moving valid images for class: ChestCT\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 2000/2000 [00:04<00:00, 429.76it/s]\n","output_type":"stream"}]},{"cell_type":"code","source":"image_size = (96,96)\nbatch_size = 32\nepochs = 10\nsteps_per_epoch = 1125\nvalid_epochs = 15\nvalid_steps_per_epoch = 200\ndevice = \"cuda\" if torch.cuda.is_available() is True else \"cpu\"\nprint(device)","metadata":{"execution":{"iopub.status.busy":"2024-06-03T12:30:00.268628Z","iopub.execute_input":"2024-06-03T12:30:00.269038Z","iopub.status.idle":"2024-06-03T12:30:00.293606Z","shell.execute_reply.started":"2024-06-03T12:30:00.269002Z","shell.execute_reply":"2024-06-03T12:30:00.292688Z"},"trusted":true},"execution_count":32,"outputs":[{"name":"stdout","text":"cuda\n","output_type":"stream"}]},{"cell_type":"code","source":"train_transform = transforms.Compose([\n    transforms.ToTensor(),\n    transforms.Resize(size = image_size,antialias = True),\n    transforms.Normalize(mean = [0.5],std = [0.5]),\n    transforms.RandomHorizontalFlip()\n])\n\nvalid_transform = transforms.Compose([\n    transforms.ToTensor(),\n    transforms.Normalize(mean = [0.5],std = [0.5]),\n    transforms.Resize(size = image_size,antialias = True),\n])","metadata":{"execution":{"iopub.status.busy":"2024-06-03T12:30:00.296103Z","iopub.execute_input":"2024-06-03T12:30:00.296397Z","iopub.status.idle":"2024-06-03T12:30:00.30954Z","shell.execute_reply.started":"2024-06-03T12:30:00.29637Z","shell.execute_reply":"2024-06-03T12:30:00.308716Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"code","source":"train_dataset = ImageFolder(root = train_path,transform = train_transform)\nvalid_dataset = ImageFolder(root = valid_path,transform = valid_transform)","metadata":{"execution":{"iopub.status.busy":"2024-06-03T12:30:00.310738Z","iopub.execute_input":"2024-06-03T12:30:00.3111Z","iopub.status.idle":"2024-06-03T12:30:00.550272Z","shell.execute_reply.started":"2024-06-03T12:30:00.311071Z","shell.execute_reply":"2024-06-03T12:30:00.549436Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"code","source":"train_dataloader = DataLoader(dataset = train_dataset,batch_size = batch_size,shuffle = True,\n                             drop_last = True)\n\nvalid_dataloader = DataLoader(dataset = valid_dataset,batch_size = batch_size,drop_last = True)","metadata":{"execution":{"iopub.status.busy":"2024-06-03T12:30:00.551633Z","iopub.execute_input":"2024-06-03T12:30:00.552248Z","iopub.status.idle":"2024-06-03T12:30:00.557692Z","shell.execute_reply.started":"2024-06-03T12:30:00.552212Z","shell.execute_reply":"2024-06-03T12:30:00.556744Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"markdown","source":"# Trainer","metadata":{}},{"cell_type":"code","source":"class Trainer:\n    def __init__(self,model,device,train_dataloader,valid_dataloader):\n        self.model = model.to(device)\n        self.device = device\n        self.train_dataloader = train_dataloader\n        self.valid_dataloader = valid_dataloader\n    \n    def train(self,epochs,loss_fn = None,optimizer = None):\n        loss_fn = nn.CrossEntropyLoss() if loss_fn is None else loss_fn\n        optimizer = torch.optim.Adam(self.model.parameters(),lr = 4.236429595039226e-05) if optimizer is None else optimizer\n        \n        parent_bar = master_bar(range(epochs))\n        for epoch in parent_bar:\n            train_batch_losses,train_batch_acc = [],[]\n            train_samples = 0\n            child_bar = progress_bar(self.train_dataloader,parent = parent_bar)\n            self.model.train()\n            \n            for train_images,train_labels in child_bar:\n                train_images,train_labels = train_images.to(self.device),train_labels.to(self.device)\n                optimizer.zero_grad(set_to_none = True)\n                probs = self.model(train_images)\n                loss = loss_fn(probs,train_labels)\n                train_batch_losses.append(round(loss.data.item() * train_images.shape[0],5))\n                _,labels = torch.max(probs,dim = 1)\n                train_samples += train_images.shape[0]\n                match_tensor = (labels == train_labels).float()\n                train_batch_acc.append(round(float(match_tensor.sum().item()),5))\n                loss.backward()\n                optimizer.step()\n                parent_bar.child.comment = f\"Train loss: {train_batch_losses[-1] / train_images.shape[0]}, train acc: {train_batch_acc[-1] / train_labels.shape[0]}\"\n            \n            train_batch_losses = round(sum(train_batch_losses) / train_samples,5)\n            train_batch_acc = round(sum(train_batch_acc) / train_samples,5)\n            \n            valid_batch_losses,valid_batch_acc = [],[]\n            valid_samples = 0\n            child_bar = progress_bar(self.valid_dataloader,parent = parent_bar)\n            self.model.eval()\n            \n            with torch.no_grad():\n                for valid_images,valid_labels in child_bar:\n                    valid_images,valid_labels = valid_images.to(self.device),valid_labels.to(self.device)\n                    probs = self.model(valid_images)\n                    loss = loss_fn(probs,valid_labels)\n                    valid_batch_losses.append(round(loss.item() * valid_images.shape[0],5))\n                    _,labels = torch.max(probs,dim = 1)\n                    valid_samples += valid_images.shape[0]\n                    match_tensor = (labels == valid_labels).float()\n                    valid_batch_acc.append(round(float(match_tensor.sum().item()),5))\n                    parent_bar.child.comment = f\"Valid loss: {valid_batch_losses[-1] / valid_images.shape[0]}, valid acc: {valid_batch_acc[-1] / valid_images.shape[0]}\"\n            \n            valid_batch_losses = round(sum(valid_batch_losses) / valid_samples,5)\n            valid_batch_acc = round(sum(valid_batch_acc) / valid_samples,5)\n            \n            parent_bar.write(f\"Epoch: {epoch + 1} / {epochs} -> Train loss: {train_batch_losses}, train acc: {train_batch_acc}, valid loss: {valid_batch_losses}, valid acc: {valid_batch_acc}\")","metadata":{"execution":{"iopub.status.busy":"2024-06-03T12:30:04.039265Z","iopub.execute_input":"2024-06-03T12:30:04.039927Z","iopub.status.idle":"2024-06-03T12:30:04.056168Z","shell.execute_reply.started":"2024-06-03T12:30:04.039894Z","shell.execute_reply":"2024-06-03T12:30:04.055176Z"},"trusted":true},"execution_count":36,"outputs":[]},{"cell_type":"code","source":"model = Resnet(image_size = image_size,num_classes = len(classes),num_conv = 3)","metadata":{"execution":{"iopub.status.busy":"2024-06-03T12:35:01.273549Z","iopub.execute_input":"2024-06-03T12:35:01.27421Z","iopub.status.idle":"2024-06-03T12:35:01.616361Z","shell.execute_reply.started":"2024-06-03T12:35:01.274177Z","shell.execute_reply":"2024-06-03T12:35:01.615361Z"},"trusted":true},"execution_count":57,"outputs":[]},{"cell_type":"code","source":"trainer = Trainer(model,device,train_dataloader,valid_dataloader)","metadata":{"execution":{"iopub.status.busy":"2024-06-03T12:35:01.617982Z","iopub.execute_input":"2024-06-03T12:35:01.618279Z","iopub.status.idle":"2024-06-03T12:35:01.670108Z","shell.execute_reply.started":"2024-06-03T12:35:01.618253Z","shell.execute_reply":"2024-06-03T12:35:01.669262Z"},"trusted":true},"execution_count":58,"outputs":[]},{"cell_type":"code","source":"trainer.train(epochs = epochs)","metadata":{"execution":{"iopub.status.busy":"2024-06-03T12:35:01.671405Z","iopub.execute_input":"2024-06-03T12:35:01.671782Z","iopub.status.idle":"2024-06-03T12:46:54.140924Z","shell.execute_reply.started":"2024-06-03T12:35:01.671747Z","shell.execute_reply":"2024-06-03T12:46:54.140007Z"},"trusted":true},"execution_count":59,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n<style>\n    /* Turns off some styling */\n    progress {\n        /* gets rid of default border in Firefox and Opera. */\n        border: none;\n        /* Needs to be in here for Safari polyfill so background images work as expected. */\n        background-size: auto;\n    }\n    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n    }\n    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n        background: #F44336;\n    }\n</style>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Epoch: 1 / 10 -> Train loss: 1.113, train acc: 0.933, valid loss: 1.05, valid acc: 0.994<p>Epoch: 2 / 10 -> Train loss: 1.058, train acc: 0.985, valid loss: 1.047, valid acc: 0.996<p>Epoch: 3 / 10 -> Train loss: 1.054, train acc: 0.989, valid loss: 1.046, valid acc: 0.998<p>Epoch: 4 / 10 -> Train loss: 1.054, train acc: 0.99, valid loss: 1.047, valid acc: 0.997<p>Epoch: 5 / 10 -> Train loss: 1.05, train acc: 0.993, valid loss: 1.046, valid acc: 0.997<p>Epoch: 6 / 10 -> Train loss: 1.052, train acc: 0.991, valid loss: 1.045, valid acc: 0.998<p>Epoch: 7 / 10 -> Train loss: 1.051, train acc: 0.993, valid loss: 1.049, valid acc: 0.995<p>Epoch: 8 / 10 -> Train loss: 1.05, train acc: 0.994, valid loss: 1.045, valid acc: 0.998<p>Epoch: 9 / 10 -> Train loss: 1.049, train acc: 0.994, valid loss: 1.047, valid acc: 0.997<p>Epoch: 10 / 10 -> Train loss: 1.049, train acc: 0.995, valid loss: 1.046, valid acc: 0.998"},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}